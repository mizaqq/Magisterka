\documentclass{SGGW-thesis-EN}
\usepackage[utf8]{inputenc}
\usepackage[backend=biber,style=numeric]{biblatex}
\addbibresource{references.bib}

\MASTERtrue
\WZIMtrue

\title{Automated Extraction and Categorization of Product Information from Receipts}
% command \Ptitle{} can be used to give the title in Polish, if this is necessary, and can be deleted if you do not need the title in Polish
\Ptitle{}
\author{Michał Zaręba}
\date{2017}
\album{196218}
\thesis{Diploma thesis in the field of}
\course{Information Science}
\promotor{dr hab.\ inż.\ Leszek Chmielewski, prof.\ SGGW}
\pworkplace{Institute of Information Technology\\Department of Artificial Intelligence}

\usepackage{hyperref}

\begin{document}
\maketitle
\statementpage
% titles and abstracts must be given in two languages - PL, EN
\abstractpage
{OCR + BERT}
{Tematem niniejszej pracy było zaimplementowanie klasy \LaTeX{}owej pozwalającej na formatowanie tekstu zgodnie z wytycznymi nałożonymi przez uczelnię. Praca zawiera dwie
główne części. Pierwsza z nich zawiera opis najważniejszych aspektów implementacji klasy. Natomiast druga część skupia się na sposobie użycia klasy przez osoby piszące prace
dyplomowe.}
{OCR, BERT, Tesseract, thesis, implementation, SGGW, Warsaw University of Life Sciences}
{OCR + BERT}
{The subject of this thesis was to implement a \LaTeX{} class that allows formatting text according to the guidelines imposed by the university. The thesis contains two main}
{LaTeX, class, thesis, implementation, SGGW, Warsaw University of Life Sciences}



\tableofcontents


\startchapterfromoddpage % niezależnie od długości spisu treści pierwszy rozdział zacznie się na nieparzystej stronie

\chapter{Introduction}

\section{Motivation}
Tracking of expenses and managing personal finances is an important aspect of modern life.
With an increasing number of daily transactions and a vast variety of products available, individuals face significant challenges in effectively monitoring their spending and managing their budgets. 
Although receipts contain valuable details that could help consumers analyze and control their expenses, 
the majority of consumers either discard receipts shortly after purchase or find it too tedious and time-consuming to analyze them manually. 
Automating the extraction and categorization of product information from receipts could significantly simplify budget tracking and provide insights into spending habits, enabling consumers to understand precisely where their money goes.
Simple and efficient way to track expenses is essential for individuals who wish to maintain a clear overview of their spending habits and make informed financial decisions. 


\section{Problem Statement}
Most existing expense-tracking solutions focus primarily on invoices, bank statements, or require manual input. 
Large corporations and organizations typically possess the necessary budgets and technical resources to implement robust, 
automated systems for extracting and categorizing expense data from structured documents such as invoices or bank statements. 
For personal use, however, the most commonly available and practical source of spending information remains paper receipts. 
Current receipt-based solutions are often limited: many tools available today are either designed exclusively for commercial purposes, 
lack support for languages other than English, or are inadequately trained to accurately process Polish-language receipts.  
Thus, there is a clear gap and a significant need for a solution that effectively automates extraction and categorization of product details from receipts, 
specifically accommodating the complexity and linguistic characteristics of the Polish language.

\section{Objectives of the Study}
This study has two primary objectives, each directly addressing the challenges identified in the problem statement:

\begin{enumerate} 
  \item Develop a robust system capable of automatically extracting structured product information (such as product names, and prices) from Polish-language receipts using Optical Character Recognition (OCR).
  \item Implement and evaluate a product categorization module based on the embeddings generated by pre-trained models, specifically BERT (Bidirectional Encoder Representations from Transformers),
  and Sentence-BERT model (Siamese transformer network) fine-tuned with own data. 
  \item The extracted embeddings serve as input to an XGBoost classifier responsible for categorizing products into predefined expense-related categories.
\end{enumerate}

These objectives will be thoroughly addressed and analyzed in subsequent chapters. 
Given the complexity of Polish-language receipts and limited availability of labeled datasets, achieving optimal results will require careful integration and fine-tuning of multiple technologies. 
Critical aspects will include the effective integration of OCR and NLP components, as well as the development of a robust classification model capable of accurately categorizing products based on their textual descriptions that might
often be ambiguous, multiple-worded, or contain spelling errors.
\newpage
\section{Scope and Limitations}

The scope of this study is limited to the development of a system capable of automatically extracting product information from receipts and categorizing these products into predefined categories. 
The system specifically targets Polish-language receipts and will be evaluated primarily on its ability to accurately extract product costs and perform correct product categorization.

\noindent \\The limitations of this study include the following:

\begin{itemize}
    \item The developed system will not include additional functionalities such as expense tracking over time, financial report generation, or integration with external personal financial management tools.
    \item The scarcity of comprehensive, labeled Polish-language receipt datasets restricts the potential accuracy and generalization capabilities of the models developed. Consequently, results may vary when encountering receipt formats or text variations not present in the training data.
    \item The OCR will be employed without utilizing spatial information or context regarding the positioning of text on receipts. Therefore, preprocessing steps such as image cropping, alignment, and noise reduction are necessary to ensure the OCR engine receives properly formatted and isolated textual inputs.
\end{itemize}

\chapter{Literature Review}

\section{Optical Character Recognition (OCR) Technologies}
Optical Character Recognition (OCR) refers to the process of converting textual information from scanned or photographed images into machine-readable formats. 

Traditional OCR techniques primarily relied on template matching, statistical classification, and structural analysis, with limited adaptability to varying fonts and noisy inputs.\cite{ocrsystems}

Modern OCR systems use deep learning models that typically combine convolutional neural networks (CNNs) for feature extraction with recurrent or attention-based architectures (e.g., LSTM, GRU) for sequence modeling. This architecture enables the system to learn complex patterns and recognize characters across varying fonts, sizes, and orientations \cite{shi2016endtoend}.

Recent OCR models further incorporate attention mechanisms, allowing the network to dynamically focus on relevant regions of the input image. Unlike standard OCR models, LayoutLM introduces explicit spatial awareness by incorporating positional embeddings of text regions, which is particularly effective for structured documents like receipts \cite{li2020layoutlm}.
While attention-based models with spatial awareness offer notable improvements in accuracy and robustness, their effectiveness remains constrained by the availability of large, annotated datasets. As a result, the OCR systems evaluated in this study—Tesseract and PaddleOCR—do not model document structure explicitly and operate at the text-line level.

Tesseract is an open-source OCR engine maintained by Google, widely adopted across various applications. It supports multiple languages, including Polish, and can be fine-tuned on domain-specific datasets for improved accuracy. Since version 4.0, it incorporates an LSTM-based recognition engine, enhancing its performance on noisy or multilingual documents \cite{smith2007overview, smith2013history}. Tesseract is highly configurable, offering control over segmentation modes, character whitelists, and language models. Fine-tuning involves training on a targeted dataset, which can significantly enhance accuracy for specific formats such as Polish receipts.
\newpage
PaddleOCR is a deep learning-based OCR framework developed by Baidu, supporting over 80 languages. It features an end-to-end pipeline using models like DBNet for detection and CRNN or SRN for recognition, making it well-suited for multilingual documents and complex layouts \cite{du2020ppocr}. Its modular and extensible architecture enables users to adapt the pipeline for specific use cases.
\section{Semantic Text Embeddings for Receipt Item Categorization}
To enable machine learning models to analyze text, the text must first be converted into a numerical format—a process known as vectorization or word embedding which is a crucial step in Natural Language Processing (NLP).
Word embeddings are fixed-length vector representations that encode the semantic meaning of words and the relationships between them. 
The words with similar meanings are represented by similar vectors in the embedding space.\cite{almeida2023wordembeddingssurvey}

There are several approaches to generating these embeddings, and this section outlines the most common methods, explaining how they work and highlighting their key differences.
Before the word embedding process, the text must be preprocessed.

Prior to embedding, text must be preprocessed. This includes normalization steps such as lemmatization or stemming, and tokenization, which segments text into individual words or tokens \cite{jurafsky2023slp3}.

Then there are several methods of generating word embeddings, which can be broadly categorized into two groups: count-based and prediction-based methods.

\subsection{Count-based Methods}
Count-based methods rely on the idea that the meaning of a word can be inferred from its co-occurrence with other words in a given context.
Those methods typically involve creating a sparse matrix, where each row and column represents a word in the vocabulary, and the values in the matrix represent the frequency of co-occurrence between pairs of words.
The two most common approaches are the Bag of Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF)\cite{jurafsky2023slp3}.

Bag of words represents a document as a vector of word counts, ignoring the order of words and their grammatical relationships.

TF-IDF model, on the other hand, assigns weights to words based on their frequency in a document relative to their frequency in the entire corpus. 
This helps to highlight important words that are more informative for the specific document.

\subsection{Prediction-based Methods}
Prediction-based methods are word representation techniques that learn embeddings by predicting words from context (or vice versa), unlike count-based methods which rely on raw frequency statistics.
The two most common prediction-based methods are Word2Vec and BERT which are based on neural networks and their representation of words is contained in dense matrix different from the sparse matrix used in count-based methods.
The dense matrixes turns out to be more efficient and effective for representing the meaning of words in a lower-dimensional space.\cite{jurafsky2023slp3}

Word2vec is a framework used for calculating a static embeddings, that mean there is a certain numerical representation for each word in vocabulary.
There are two architectures for training Word2Vec models: Continuous Bag of Words (CBOW) and Skip-Gram.
\begin{enumerate}
  \item \textbf{Continuous Bag of Words (CBOW)}: predicts the target word from surrounding context words.
  \item \textbf{Skip-Gram} Skip-Gram: predicts context words from a single target word.
\end{enumerate}
Skip-Gram is particularly effective for representing rare words, as it captures word–context associations more accurately,
whereas CBOW offers greater computational efficiency and faster training times \cite{mikolov2013distributed}.

BERT (Bidirectional Encoder Representations from Transformers) is a deep learning model introduced by Google that generates contextualized word embeddings by processing text in both directions (left-to-right and right-to-left) simultaneously. 
Unlike earlier models that produce static embeddings, BERT captures the meaning of a word based on its entire sentence context. 
Transformer architecture, the backbone of BERT, employs self-attention mechanisms to weight the importance of words in relation to each other, allowing the model to derivate complex semantic relationships between them.
To achieve this, BERT is pretrained using a two-step self-supervised training process:
\begin{enumerate}
  \item \textbf{Masked Language Model (MLM)}: Randomly masks a percentage of input tokens and trains the model to predict the masked words based on context.
  \newpage
  \item \textbf{Next Sentence Prediction (NSP)}: Trains the model to predict whether two sentences are follow each other or not, improving its understanding of sentences coherence.
\end{enumerate}
These context-aware embeddings have demonstrated strong performance across various NLP tasks, including classification.\cite{devlin2019bertpretrainingdeepbidirectional}.
In this study, embeddings generated by pre-trained BERT models serve input features for classification task which will categorize products into predefined expense-related categories.

\section{XGBoost Classifier}
XGBoost stands for eXtreme Gradient Boosting and is an open-source machine learning library that implements 
the gradient boosting framework for decision trees. It is an state-of-the-art algorith for supervised learning that achives best performance among many machine learning challanges.\cite{Chen_2016}
Gradient boosting is an ensemble learning technique that instead of training a single model, build an initial model 
and then iteratively fits new models through loss function minimization.\cite{natekin2013gradient}
XGBoost model use set of rules in tree generation.
\begin{enumerate}
\item Regularization which is used to prevent too complex trees and tends to choose the most predictive ones while also minimizing overfitting.
\item Gradient Tree Boosting introduces a second-order Taylor approximation to the loss function, incorporating both the first (gradient) and second (Hessian) derivatives. This allows accurate estimation while ensuring high efficiency of the algorithm.
\item Shrinkage is a technique that after each boosting step, the prediction scores are multiplied by a learning rate parameter. This prevent overfitting and allows for more robust models that learn more slowy and carefully. 
\item Column Subsampling: parameter that derives from random forest algorithm, randomly selects part of features for each tree. 
\end{enumerate} 

\section{Integration of OCR and NLP}
[...]

\chapter{Methodology}

\section{System Architecture}
[...]

\section{Data Collection and Preprocessing}
[...]

\section{Optical Character Recognition with Tesseract}
[...]

\section{Text Parsing and Information Extraction}
[...]

\section{Product Categorization Using BERT}
[...]

\section{Data Grouping and Organization}
[...]

\chapter{Implementation}

\section{Development Environment}
[...]

\section{Integration of Tesseract OCR}
[...]

\section{BERT Model Fine-Tuning}
[...]

\section{System Workflow}
[...]

\chapter{Evaluation and Results}

\section{Evaluation Metrics}
[...]

\section{Experimental Setup}
[...]

\section{Results and Analysis}
[...]

\section{Comparison with Existing Methods}
[...]

\chapter{Discussion}

\section{Interpretation of Results}
[...]

\section{Challenges and Limitations}
[...]

\section{Recommendations for Future Work}
[...]

\chapter{Conclusion}

\section{Summary of Findings}
[...]

\section{Contributions to the Field}
[...]

\section{Final Remarks}
[...]

\chapter{References}
[...]

\chapter{Appendices}

\section{Sample Receipt Data}
[...]

\section{Code Snippets}
[...]

\section{Additional Figures and Tables}
[...]
\renewcommand{\bibname}{Bibliography}

\printbibliography

\beforelastpage

\end{document}
