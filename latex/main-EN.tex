\documentclass{SGGW-thesis-EN}
\usepackage[utf8]{inputenc}
\usepackage[backend=biber,style=numeric]{biblatex}
\addbibresource{references.bib}

\MASTERtrue
\WZIMtrue

\title{Automated Extraction and Categorization of Product Information from Receipts}
% command \Ptitle{} can be used to give the title in Polish, if this is necessary, and can be deleted if you do not need the title in Polish
\Ptitle{}
\author{Michał Zaręba}
\date{2017}
\album{196218}
\thesis{Diploma thesis in the field of}
\course{Information Science}
\promotor{dr hab.\ inż.\ Leszek Chmielewski, prof.\ SGGW}
\pworkplace{Institute of Information Technology\\Department of Artificial Intelligence}

\usepackage{hyperref}

\begin{document}
\maketitle
\statementpage
% titles and abstracts must be given in two languages - PL, EN
\abstractpage
{OCR + BERT}
{Tematem niniejszej pracy było zaimplementowanie klasy \LaTeX{}owej pozwalającej na formatowanie tekstu zgodnie z wytycznymi nałożonymi przez uczelnię. Praca zawiera dwie
główne części. Pierwsza z nich zawiera opis najważniejszych aspektów implementacji klasy. Natomiast druga część skupia się na sposobie użycia klasy przez osoby piszące prace
dyplomowe.}
{OCR, BERT, Tesseract, thesis, implementation, SGGW, Warsaw University of Life Sciences}
{OCR + BERT}
{The subject of this thesis was to implement a \LaTeX{} class that allows formatting text according to the guidelines imposed by the university. The thesis contains two main}
{LaTeX, class, thesis, implementation, SGGW, Warsaw University of Life Sciences}



\tableofcontents


\startchapterfromoddpage % niezależnie od długości spisu treści pierwszy rozdział zacznie się na nieparzystej stronie

\chapter{Introduction}

\section{Motivation}
Tracking of expenses and managing personal finances is an important aspect of modern life.
With an increasing number of daily transactions and a vast variety of products available, individuals face significant challenges in effectively monitoring their spending and managing their budgets. 
Although receipts contain valuable details that could help consumers analyze and control their expenses, 
the majority of consumers either discard receipts shortly after purchase or find it too tedious and time-consuming to analyze them manually. 
Automating the extraction and categorization of product information from receipts could significantly simplify budget tracking and provide insights into spending habits, enabling consumers to understand precisely where their money goes.
Simple and efficient way to track expenses is essential for individuals who wish to maintain a clear overview of their spending habits and make informed financial decisions. 


\section{Problem Statement}
Most existing expense-tracking solutions focus primarily on invoices, bank statements, or require manual input. 
Large corporations and organizations typically possess the necessary budgets and technical resources to implement robust, 
automated systems for extracting and categorizing expense data from structured documents such as invoices or bank statements. 
For personal use, however, the most commonly available and practical source of spending information remains paper receipts. 
Current receipt-based solutions are often limited: many tools available today are either designed exclusively for commercial purposes, 
lack support for languages other than English, or are inadequately trained to accurately process Polish-language receipts.  
Thus, there is a clear gap and a significant need for a solution that effectively automates extraction and categorization of product details from receipts, 
specifically accommodating the complexity and linguistic characteristics of the Polish language.

\section{Objectives of the Study}
This study has two primary objectives, each directly addressing the challenges identified in the problem statement:

\begin{enumerate} 
  \item Develop a robust system capable of automatically extracting structured product information (such as product names, and prices) from Polish-language receipts using Optical Character Recognition (OCR).
  \item Implement and evaluate a product categorization module based on the embeddings generated by pre-trained models, specifically BERT (Bidirectional Encoder Representations from Transformers),
  and Sentence-BERT model (Siamese transformer network) fine-tuned with own data. 
  \item The extracted embeddings serve as input to an XGBoost classifier responsible for categorizing products into predefined expense-related categories.
\end{enumerate}

These objectives will be thoroughly addressed and analyzed in subsequent chapters. 
Given the complexity of Polish-language receipts and limited availability of labeled datasets, achieving optimal results will require careful integration and fine-tuning of multiple technologies. 
Critical aspects will include the effective integration of OCR and NLP components, as well as the development of a robust classification model capable of accurately categorizing products based on their textual descriptions that might
often be ambiguous, multiple-worded, or contain spelling errors.
\newpage
\section{Scope and Limitations}

The scope of this study is limited to the development of a system capable of automatically extracting product information from receipts and categorizing these products into predefined categories. 
The system specifically targets Polish-language receipts and will be evaluated primarily on its ability to accurately extract product costs and perform correct product categorization.

\noindent \\The limitations of this study include the following:

\begin{itemize}
    \item The developed system will not include additional functionalities such as expense tracking over time, financial report generation, or integration with external personal financial management tools.
    \item The scarcity of comprehensive, labeled Polish-language receipt datasets restricts the potential accuracy and generalization capabilities of the models developed. Consequently, results may vary when encountering receipt formats or text variations not present in the training data.
    \item The OCR will be employed without utilizing spatial information or context regarding the positioning of text on receipts. Therefore, preprocessing steps such as image cropping, alignment, and noise reduction are necessary to ensure the OCR engine receives properly formatted and isolated textual inputs.
\end{itemize}

\chapter{Literature Review}

\section{Optical Character Recognition (OCR) Technologies}
Optical Character Recognition (OCR) refers to the process of converting textual information from scanned or photographed images into machine-readable formats. 

Traditional OCR techniques primarily relied on template matching, statistical classification, and structural analysis, with limited adaptability to varying fonts and noisy inputs.\cite{ocrsystems}

Modern OCR systems use deep learning models that typically combine convolutional neural networks (CNNs) for feature extraction with recurrent or attention-based architectures (e.g., LSTM, GRU) for sequence modeling. This architecture enables the system to learn complex patterns and recognize characters across varying fonts, sizes, and orientations \cite{shi2016endtoend}.

Recent OCR models further incorporate attention mechanisms, allowing the network to dynamically focus on relevant regions of the input image. Unlike standard OCR models, LayoutLM introduces explicit spatial awareness by incorporating positional embeddings of text regions, which is particularly effective for structured documents like receipts \cite{li2020layoutlm}.
While attention-based models with spatial awareness offer notable improvements in accuracy and robustness, their effectiveness remains constrained by the availability of large, annotated datasets. As a result, the OCR systems evaluated in this study—Tesseract and PaddleOCR—do not model document structure explicitly and operate at the text-line level.

Tesseract is an open-source OCR engine maintained by Google, widely adopted across various applications. It supports multiple languages, including Polish, and can be fine-tuned on domain-specific datasets for improved accuracy. Since version 4.0, it incorporates an LSTM-based recognition engine, enhancing its performance on noisy or multilingual documents \cite{smith2007overview, smith2013history}. Tesseract is highly configurable, offering control over segmentation modes, character whitelists, and language models. Fine-tuning involves training on a targeted dataset, which can significantly enhance accuracy for specific formats such as Polish receipts.
\newpage
PaddleOCR is a deep learning-based OCR framework developed by Baidu, supporting over 80 languages. It features an end-to-end pipeline using models like DBNet for detection and CRNN or SRN for recognition, making it well-suited for multilingual documents and complex layouts \cite{du2020ppocr}. Its modular and extensible architecture enables users to adapt the pipeline for specific use cases.
\section{Semantic Text Embeddings for Receipt Item Categorization}
For a system to accurately perform any analysis on text, it must first convert the text into a numerical representation, 
which then can be used as input to a machine learning model. 
This process is known as vectorization or word embedding, and it is a crucial step in Natural Language Processing (NLP).
Word embeddings are fixed-length vector representations that encode the semantic meaning of words and the relationships between them. It is a stated fact that words with similar meanings are represented by similar vectors in the embedding space.\cite{almeida2023wordembeddingssurvey}
There are several approaches to generating these embeddings, and this section outlines the most common methods, explaining how they work and highlighting their key differences.
Before the word embedding process, the text must be preprocessed.
This includes data normalization to process words into their base state (e.g by lemmatization,stemming and many more)\cite{jurafsky2023slp3} and tokenization, which is the process of splitting the text into individual words or tokens.

Then there are several methods of generating word embeddings, which can be broadly categorized into two groups: count-based and prediction-based methods.

\subsection{Count-based Methods}
Count-based methods are based on the idea that the meaning of a word can be inferred from its co-occurrence with other words in a given context.
Those methods typically involve creating a sparse matrix, where each row and column represents a word in the vocabulary, and the values in the matrix represent the frequency of co-occurrence between pairs of words.
The two most common count-based methods are the Bag of Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF)\cite{jurafsky2023slp3} models.

The bag of words model represents a document as a vector of word counts, ignoring the order of words and their grammatical relationships.

The TF-IDF model, on the other hand, assigns weights to words based on their frequency in a document relative to their frequency in the entire corpus. This helps to highlight important words that are more informative for the specific document.

\subsection{Prediction-based Methods}
Prediction-based methods are word representation techniques that learn embeddings by predicting words from context (or vice versa), unlike count-based methods which rely on raw frequency statistics.
The two most common prediction-based methods are Word2Vec and BERT which are based on neural networks and their representation of words is contained in dense matrix different from the sparse matrix used in count-based methods.
The dense matrixes turns out to be more efficient and effective for representing the meaning of words in a lower-dimensional space.\cite{jurafsky2023slp3}

Word2vec is calculating a static embeddings, that mean there is a certain numerical representation for each word in vocabulary.
That is achieved by two architectures: Continuous Bag of Words (CBOW) and Skip-Gram.

CBOW predicts a target word based on its context, while Skip-Gram predicts the context based on a target word.
The Skip-Gram model is particularly effective for infrequent words, as it captures the relationships between words in a more nuanced way than CBOW.



[...]


\section{Integration of OCR and NLP}
[...]

\chapter{Methodology}

\section{System Architecture}
[...]

\section{Data Collection and Preprocessing}
[...]

\section{Optical Character Recognition with Tesseract}
[...]

\section{Text Parsing and Information Extraction}
[...]

\section{Product Categorization Using BERT}
[...]

\section{Data Grouping and Organization}
[...]

\chapter{Implementation}

\section{Development Environment}
[...]

\section{Integration of Tesseract OCR}
[...]

\section{BERT Model Fine-Tuning}
[...]

\section{System Workflow}
[...]

\chapter{Evaluation and Results}

\section{Evaluation Metrics}
[...]

\section{Experimental Setup}
[...]

\section{Results and Analysis}
[...]

\section{Comparison with Existing Methods}
[...]

\chapter{Discussion}

\section{Interpretation of Results}
[...]

\section{Challenges and Limitations}
[...]

\section{Recommendations for Future Work}
[...]

\chapter{Conclusion}

\section{Summary of Findings}
[...]

\section{Contributions to the Field}
[...]

\section{Final Remarks}
[...]

\chapter{References}
[...]

\chapter{Appendices}

\section{Sample Receipt Data}
[...]

\section{Code Snippets}
[...]

\section{Additional Figures and Tables}
[...]
\renewcommand{\bibname}{Bibliography}

\printbibliography

\beforelastpage

\end{document}
