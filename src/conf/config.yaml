params:
  learning_rate: 0.1
  max_depth: 5
  n_estimators: 500
  subsample: 0.5
  colsample_bytree: 0.7
  min_child_weight: 1
  gamma: 0.0
  reg_alpha: 0.0
  reg_lambda: 0.1

embeddingmodel:
  model_path: "bert"
gptdatapath: '/home/miza/Magisterka/src/data/annotations/annotations_gpt.csv'
datapath: '/home/miza/Magisterka/src/data/annotations/annotations_6classes.csv'
balance_data: True
split_size: 0.3
defaults:
  - _self_
  - params: params_best_bert
  - override hydra/sweeper: optuna

finetune: 
  finetune: false
  model_base: '/home/miza/Magisterka/src/models/mini/finetuned_base_model'
  model_augmented: '/home/miza/Magisterka/src/models/mini/finetuned_augmented_model'

hydra:
  sweeper:
    direction: maximize
    study_name: xgb_sweep
    n_trials: 100
    n_jobs: 4
    sampler:
      _target_: optuna.samplers.TPESampler
      seed: 42
    params:
      params.learning_rate: float(0.01, 0.05, 0.1, 0.2, 0.3)
      params.max_depth: int(5, 5, 7, 10, 15, 20)
      params.n_estimators: int(50, 100, 200, 300, 500, 750, 1000)
      params.subsample: float(0.5, 0.6, 0.7, 0.8, 0.9, 1.0)
      params.colsample_bytree: float(0.5, 0.6, 0.7, 0.8, 0.9, 1.0)
      params.min_child_weight: int(1, 2, 3, 5, 7, 10)
      params.gamma: float(0.0, 0.1, 0.5, 1.0, 2.0, 5.0)
      params.reg_alpha: float(0.0, 0.1, 0.5, 1.0, 2.0, 5.0)
      params.reg_lambda: float(0.0, 0.1, 0.5, 1.0, 2.0, 5.0)
      embeddingmodel.model_path: str('bert')


