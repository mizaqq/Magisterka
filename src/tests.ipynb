{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Line_Text\n",
      "0                                       \n",
      "1     Tabl Plusssz 203zt B 4 x6,69 6,69B\n",
      "2        Tost Maślany a D 8 «4,28 34,240\n",
      "3    Jost pełnozia 500g l 8 x3,19 25,520\n",
      "4     Woda Niegaz 1,51 A 12 x0,39 11,88R\n",
      "5  HodaNGaz Muszyf ,5L A 6 x2,85 17, 10Ą\n",
      "6    lap CocalolaZeroŻL [i 1 x8,69 8,69A\n"
     ]
    }
   ],
   "source": [
    "from pytesseract import pytesseract, Output\n",
    "import cv2\n",
    "import pandas as pd\n",
    "# Load the receipt image\n",
    "image = cv2.imread('/home/miza/Magisterka/src/data/paragon3.jpg')\n",
    "\n",
    "# Image Preprocessing\n",
    "# Convert the image to grayscale\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply Gaussian blur to reduce noise\n",
    "blurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0)\n",
    "\n",
    "# Apply binary thresholding (Binarization)\n",
    "_, threshold_image = cv2.threshold(blurred_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "# Optional: Resize image for better OCR accuracy (if necessary)\n",
    "height, width = threshold_image.shape\n",
    "threshold_image = cv2.resize(threshold_image, (width * 2, height * 2))\n",
    "# Extract text with bounding boxes\n",
    "ocr_data = pytesseract.image_to_data(image, output_type=Output.DICT, config = '--psm 6', lang='pol')\n",
    "\n",
    "lines = []\n",
    "current_line = []\n",
    "\n",
    "# Iterate through OCR results and group words into lines\n",
    "prev_line_number = ocr_data['line_num'][0]\n",
    "for i in range(len(ocr_data['text'])):\n",
    "    word = ocr_data['text'][i]\n",
    "    line_number = ocr_data['line_num'][i]\n",
    "\n",
    "    if word.strip():  # Skip empty words\n",
    "        if line_number != prev_line_number:  # New line detected\n",
    "            lines.append(\" \".join(current_line))  # Append the current line\n",
    "            current_line = []  # Start a new line\n",
    "            prev_line_number = line_number\n",
    "\n",
    "        current_line.append(word)\n",
    "\n",
    "# Append the last line if there is one\n",
    "if current_line:\n",
    "    lines.append(\" \".join(current_line))\n",
    "\n",
    "# Create a DataFrame from the lines\n",
    "df = pd.DataFrame(lines, columns=[\"Line_Text\"])\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForTokenClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Sample dataset structure\n",
    "data = {\n",
    "    'Text': [\n",
    "        \"SALATKA JARZY 250g-D\",\n",
    "        \"JOG ALE PITNY 290g-0\",\n",
    "        \"KAJZERKA PREM 60g-D \",\n",
    "        \"KAJZERKA PREM 60g-D \"\n",
    "    ],\n",
    "    'Category': ['Food', 'Beverage', 'Food', 'Food'],\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Tokenizer initialization\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize text\n",
    "inputs = tokenizer(df['Text'].tolist(), padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Example labels (for classification task, need to convert to numeric values)\n",
    "label_mapping = {'Food': 0, 'Beverage': 1}  # Mapping categories to numeric values\n",
    "labels = [label_mapping[label] for label in df['Category']]\n",
    "\n",
    "# Pad the labels to match the sequence length of the inputs (important for token classification)\n",
    "labels_padded = []\n",
    "for i, label in enumerate(labels):\n",
    "    # Create a label tensor of the same length as the tokenized input\n",
    "    label_tensor = torch.tensor([label] * len(inputs['input_ids'][i]))\n",
    "    labels_padded.append(label_tensor)\n",
    "\n",
    "# Stack the labels so they form a tensor with the correct shape\n",
    "labels_padded = torch.stack(labels_padded)\n",
    "\n",
    "# Add labels to inputs\n",
    "inputs['labels'] = labels_padded\n",
    "\n",
    "# Convert inputs to Dataset format\n",
    "dataset = Dataset.from_dict(inputs)\n",
    "\n",
    "# Initialize BERT model for token classification\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=len(label_mapping))\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,  # Use dataset here\n",
    "    eval_dataset=dataset     # You should use a separate validation set for evaluation\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# After training, you can use the model for prediction\n",
    "outputs = model(**inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "['Food', 'Food', 'Food', 'Food', 'Food', 'Food', 'Food', 'Food', 'Food', 'Food', 'Food']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# Assuming 'outputs' is the TokenClassifierOutput object\n",
    "logits = outputs.logits  # Access the logits from the output\n",
    "\n",
    "# Apply softmax to the logits to get probabilities\n",
    "probabilities = F.softmax(logits, dim=-1)\n",
    "predictions = probabilities.argmax(dim=-1)\n",
    "# Print the probabilities\n",
    "print(predictions)\n",
    "# Example class mapping\n",
    "label_mapping = {0: \"Food\", 1: \"Beverage\"}\n",
    "\n",
    "# Map the predictions to class labels\n",
    "predicted_labels = [label_mapping[pred.item()] for pred in predictions[0]]\n",
    "\n",
    "# Print the predicted labels for each token\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Tabl Plusssz\n",
      "Tokens: ['[CLS]', 'tab', '##l', 'plus', '##ss', '##z', '[SEP]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n",
      "Original: Tost Maślany\n",
      "Tokens: ['[CLS]', 'to', '##st', 'mas', '##lan', '##y', '[SEP]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n",
      "Original: Jost pełnozia\n",
      "Tokens: ['[CLS]', 'jo', '##st', 'pe', '##ł', '##no', '##zia', '[SEP]', '[PAD]', '[PAD]']\n",
      "\n",
      "Original: Woda Niegaz\n",
      "Tokens: ['[CLS]', 'wo', '##da', 'ni', '##ega', '##z', '[SEP]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n",
      "Original: HodaNGaz Muszyf\n",
      "Tokens: ['[CLS]', 'ho', '##dan', '##ga', '##z', 'mu', '##sz', '##y', '##f', '[SEP]']\n",
      "\n",
      "Original: lap CocalolaZeroŻL\n",
      "Tokens: ['[CLS]', 'lap', 'coca', '##lo', '##laze', '##ro', '##z', '##l', '[SEP]', '[PAD]']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'Text': [\n",
    "        \"Tabl Plusssz\",\n",
    "        \"Tost Maślany\",\n",
    "        \"Jost pełnozia\",\n",
    "        \"Woda Niegaz\",\n",
    "        \"HodaNGaz Muszyf\",\n",
    "        \"lap CocalolaZeroŻL\"\n",
    "    ]\n",
    "}\n",
    "inputs = tokenizer(data['Text'], padding=True, truncation=True, return_tensors='pt')\n",
    "for sentence, input_ids in zip(data['Text'], inputs['input_ids']):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)  # Convert IDs to tokens\n",
    "    print(f\"Original: {sentence}\")\n",
    "    print(f\"Tokens: {tokens}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "['Food', 'Food', 'Food', 'Beverage', 'Food', 'Food', 'Food', 'Food', 'Food', 'Food']\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs)\n",
    "logits = outputs.logits  # Access the logits from the output\n",
    "\n",
    "# Apply softmax to the logits to get probabilities\n",
    "probabilities = F.softmax(logits, dim=-1)\n",
    "predictions = probabilities.argmax(dim=-1)\n",
    "# Print the probabilities\n",
    "print(predictions)\n",
    "# Example class mapping\n",
    "label_mapping = {0: \"Food\", 1: \"Beverage\"}\n",
    "\n",
    "# Map the predictions to class labels\n",
    "predicted_labels = [label_mapping[pred.item()] for pred in predictions[0]]\n",
    "\n",
    "# Print the predicted labels for each token\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magisterka-eeyuwOSn-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
